{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSC140A SuperHW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem. Consider the words “meilleur” and “mejor”. In English, both of these words mean “best”\n",
    "– one in French and the other in Spanish. Even if you don’t know how to speak either of these languages,\n",
    "you might be able to guess which word is Spanish and which is French based on the spelling. For example,\n",
    "the word “meilleur” looks more like a French word than a Spanish word due to it containing “ei” and ending\n",
    "in “eur”. On the other hand, the word “mejor” looks more like a Spanish word than a French word due to it\n",
    "containing “j” and ending in “or”. This suggests that there is some statistical structure in the words of these\n",
    "languages that we can use to build a machine learning model capable of distinguishing between French and\n",
    "Spanish without actually understanding the words themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal in this problem is to build a machine learning model that can take a word as input and predict\n",
    "whether it is Spanish or French."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• train_words: a list of n strings, each one of them a word (in either Spanish or French)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• train_labels: a list of n strings, each one of them either \"spanish\" or \"french\", indicating the\n",
    "language of the corresponding word in train_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•  test_words: a list of m strings, each one of them a word (in either Spanish or French)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function should return a list of m strings, each one of them either \"spanish\" or \"french\", indicating\n",
    "your classifier’s prediction for the language of the corresponding word in test_words.\n",
    "Your classify() function is responsible for training your machine learning model on the training data and\n",
    "then using that model to make predictions on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good choice of features is important. You might want to consider using the frequency of different\n",
    "letters or pairs of letters in each word as features. For example, one of your features might be whether\n",
    "“el” appears in the word. You do not need to create all of these features by hand – you can use Python\n",
    "to help you generate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don’t confuse training accuracy with test accuracy. It is possible to achieve 90%+ training accuracy\n",
    "on this data set, but that doesn’t mean your model will generalize well to the test set.\n",
    "2\n",
    "• Be careful to avoid overfitting! If you use too many features or too complex of a model, you may find\n",
    "that your model performs well on the training data but poorly on the test data.\n",
    "• Start with the simplest models first. We have learned some models in this class that can be implemented\n",
    "using only a couple lines of code (with numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def classify(train_words, train_labels, test_words):\n",
    "    \"\"\"\n",
    "    Classify words as either Spanish or French\n",
    "    \n",
    "    Parameters:\n",
    "    - train_words: List of strings (words) for training\n",
    "    - train_labels: List of strings (either \"spanish\" or \"french\") matching train_words\n",
    "    - test_words: List of strings (words) to classify\n",
    "    \n",
    "    Returns:\n",
    "    - List of predicted labels (\"spanish\" or \"french\") for test_words\n",
    "    \"\"\"\n",
    "    # Implement a Naive Bayes classifier with character n-gram features\n",
    "    \n",
    "    # Split training data by class\n",
    "    spanish_words = [word.lower() for word, label in zip(train_words, train_labels) if label == \"spanish\"]\n",
    "    french_words = [word.lower() for word, label in zip(train_words, train_labels) if label == \"french\"]\n",
    "    \n",
    "    # Get prior probabilities\n",
    "    total_words = len(train_words)\n",
    "    p_spanish = len(spanish_words) / total_words\n",
    "    p_french = len(french_words) / total_words\n",
    "    \n",
    "    # Extract features: single characters, bigrams, and ending patterns\n",
    "    spanish_chars = extract_features(spanish_words)\n",
    "    french_chars = extract_features(french_words)\n",
    "    \n",
    "    # Calculate the sum of feature counts for each class\n",
    "    spanish_total = sum(spanish_chars.values())\n",
    "    french_total = sum(french_chars.values())\n",
    "    \n",
    "    # Get all unique features\n",
    "    all_features = set(list(spanish_chars.keys()) + list(french_chars.keys()))\n",
    "    feature_count = len(all_features)\n",
    "    \n",
    "    # Make predictions for each test word\n",
    "    predictions = []\n",
    "    for word in test_words:\n",
    "        word = word.lower()\n",
    "        \n",
    "        # Calculate log probabilities for each class (using log to avoid underflow)\n",
    "        log_p_spanish = np.log(p_spanish)\n",
    "        log_p_french = np.log(p_french)\n",
    "        \n",
    "        # Extract features from the test word\n",
    "        word_features = get_word_features(word)\n",
    "        \n",
    "        # Update probabilities based on features\n",
    "        for feature in word_features:\n",
    "            # Handle Spanish probability (with Laplace smoothing)\n",
    "            if feature in spanish_chars:\n",
    "                log_p_spanish += np.log((spanish_chars[feature] + 1) / (spanish_total + feature_count))\n",
    "            else:\n",
    "                log_p_spanish += np.log(1 / (spanish_total + feature_count))\n",
    "            \n",
    "            # Handle French probability (with Laplace smoothing)\n",
    "            if feature in french_chars:\n",
    "                log_p_french += np.log((french_chars[feature] + 1) / (french_total + feature_count))\n",
    "            else:\n",
    "                log_p_french += np.log(1 / (french_total + feature_count))\n",
    "        \n",
    "        # Add slight bias toward French to fix our imbalance from analysis\n",
    "        log_p_french += 0.05\n",
    "        \n",
    "        # Make prediction based on higher probability\n",
    "        if log_p_spanish > log_p_french:\n",
    "            predictions.append(\"spanish\")\n",
    "        else:\n",
    "            predictions.append(\"french\")\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def extract_features(words):\n",
    "    \"\"\"\n",
    "    Extract character-level features from a list of words\n",
    "    \"\"\"\n",
    "    features = Counter()\n",
    "    \n",
    "    for word in words:\n",
    "        # Add single characters\n",
    "        for char in word:\n",
    "            features[f\"CHAR_{char}\"] += 1\n",
    "        \n",
    "        # Add bigrams\n",
    "        for i in range(len(word) - 1):\n",
    "            bigram = word[i:i+2]\n",
    "            features[f\"BI_{bigram}\"] += 1\n",
    "        \n",
    "        # Add trigrams (very distinctive for language identification)\n",
    "        for i in range(len(word) - 2):\n",
    "            trigram = word[i:i+3]\n",
    "            features[f\"TRI_{trigram}\"] += 1\n",
    "        \n",
    "        # Add special features for word endings (very important for language identification)\n",
    "        for length in [1, 2, 3]:\n",
    "            if len(word) >= length:\n",
    "                ending = word[-length:]\n",
    "                features[f\"END_{ending}\"] += 3  # Give more weight to endings\n",
    "        \n",
    "        # Add special features for word beginnings\n",
    "        for length in [1, 2, 3]:\n",
    "            if len(word) >= length:\n",
    "                beginning = word[:length]\n",
    "                features[f\"START_{beginning}\"] += 2  # Give more weight to beginnings\n",
    "                \n",
    "        # Add word length as a feature\n",
    "        length_range = min(len(word) // 2, 5)  # Group lengths to avoid overfitting\n",
    "        features[f\"LEN_{length_range}\"] += 1\n",
    "        \n",
    "        # Add common French patterns with higher weight\n",
    "        french_patterns = [\"eu\", \"ou\", \"ai\", \"ei\", \"au\", \"eau\", \"oi\", \"ie\", \"tion\", \"eux\", \"aux\"]\n",
    "        for pattern in french_patterns:\n",
    "            if pattern in word:\n",
    "                features[f\"FR_PATTERN_{pattern}\"] += 3\n",
    "        \n",
    "        # Add common Spanish patterns with higher weight\n",
    "        spanish_patterns = [\"os\", \"ar\", \"er\", \"ir\", \"mente\", \"dad\", \"cion\", \"ll\", \"rr\", \"ia\", \"io\"]\n",
    "        for pattern in spanish_patterns:\n",
    "            if pattern in word:\n",
    "                features[f\"ES_PATTERN_{pattern}\"] += 3\n",
    "    \n",
    "    return features\n",
    "\n",
    "def get_word_features(word):\n",
    "    \"\"\"\n",
    "    Extract features from a single word\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Add single characters\n",
    "    for char in word:\n",
    "        features.append(f\"CHAR_{char}\")\n",
    "    \n",
    "    # Add bigrams\n",
    "    for i in range(len(word) - 1):\n",
    "        bigram = word[i:i+2]\n",
    "        features.append(f\"BI_{bigram}\")\n",
    "    \n",
    "    # Add special features for word endings\n",
    "    for length in [1, 2, 3]:\n",
    "        if len(word) >= length:\n",
    "            ending = word[-length:]\n",
    "            features.append(f\"END_{ending}\")\n",
    "    \n",
    "    # Add special features for word beginnings\n",
    "    for length in [1, 2, 3]:\n",
    "        if len(word) >= length:\n",
    "            beginning = word[:length]\n",
    "            features.append(f\"START_{beginning}\")\n",
    "            \n",
    "    # Add word length as a feature\n",
    "    length_range = min(len(word) // 2, 5)\n",
    "    features.append(f\"LEN_{length_range}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated accuracy: 0.8469 ± 0.0473\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def evaluate_classifier(all_words, all_labels, k=5):\n",
    "    \"\"\"Perform k-fold cross-validation to estimate model accuracy\"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    accuracies = []\n",
    "    \n",
    "    for train_idx, test_idx in kf.split(all_words):\n",
    "        train_words = [all_words[i] for i in train_idx]\n",
    "        train_labels = [all_labels[i] for i in train_idx]\n",
    "        test_words = [all_words[i] for i in test_idx]\n",
    "        test_labels = [all_labels[i] for i in test_idx]\n",
    "        \n",
    "        predictions = classify(train_words, train_labels, test_words)\n",
    "        accuracy = sum(p == t for p, t in zip(predictions, test_labels)) / len(test_labels)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return np.mean(accuracies), np.std(accuracies)\n",
    "\n",
    "# Usage\n",
    "mean_acc, std_acc = evaluate_classifier(train_words, train_labels)\n",
    "print(f\"Estimated accuracy: {mean_acc:.4f} ± {std_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_feature_importance(train_words, train_labels):\n",
    "    \"\"\"Identify which features best distinguish between languages\"\"\"\n",
    "    spanish_words = [w for w, l in zip(train_words, train_labels) if l == \"spanish\"]\n",
    "    french_words = [w for w, l in zip(train_words, train_labels) if l == \"french\"]\n",
    "    \n",
    "    spanish_features = extract_features(spanish_words)\n",
    "    french_features = extract_features(french_words)\n",
    "    \n",
    "    # Normalize counts to frequencies\n",
    "    total_spanish = sum(spanish_features.values())\n",
    "    total_french = sum(french_features.values())\n",
    "    \n",
    "    important_features = []\n",
    "    for feature in set(list(spanish_features.keys()) + list(french_features.keys())):\n",
    "        sp_freq = spanish_features.get(feature, 0) / total_spanish\n",
    "        fr_freq = french_features.get(feature, 0) / total_french\n",
    "        importance = abs(sp_freq - fr_freq)\n",
    "        important_features.append((feature, importance, sp_freq, fr_freq))\n",
    "    \n",
    "    return sorted(important_features, key=lambda x: x[1], reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(train_words, train_labels, test_words, test_labels):\n",
    "    \"\"\"Generate confusion matrix to identify error patterns\"\"\"\n",
    "    predictions = classify(train_words, train_labels, test_words)\n",
    "    \n",
    "    # Initialize counts\n",
    "    tp = fp = tn = fn = 0\n",
    "    for pred, true in zip(predictions, test_labels):\n",
    "        if pred == \"spanish\" and true == \"spanish\":\n",
    "            tp += 1\n",
    "        elif pred == \"spanish\" and true == \"french\":\n",
    "            fp += 1\n",
    "        elif pred == \"french\" and true == \"french\":\n",
    "            tn += 1\n",
    "        elif pred == \"french\" and true == \"spanish\":\n",
    "            fn += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / len(test_labels)\n",
    "    precision_spanish = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_spanish = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_spanish\": precision_spanish,\n",
    "        \"recall_spanish\": recall_spanish,\n",
    "        \"confusion_matrix\": [[tp, fn], [fp, tn]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_edge_cases(train_words, train_labels):\n",
    "    \"\"\"Test classifier on specially crafted challenging cases\"\"\"\n",
    "    edge_cases = [\n",
    "        # Words that could be either language\n",
    "        \"animal\", \"central\", \"radio\", \"normal\",\n",
    "        \n",
    "        # Very short words\n",
    "        \"no\", \"si\", \"la\", \"le\",\n",
    "        \n",
    "        # Words with language-specific patterns\n",
    "        \"español\", \"français\", \"biblioteca\", \"bibliothèque\",\n",
    "    ]\n",
    "    \n",
    "    predictions = classify(train_words, train_labels, edge_cases)\n",
    "    for word, pred in zip(edge_cases, predictions):\n",
    "        print(f\"'{word}' classified as {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
