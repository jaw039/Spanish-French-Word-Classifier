{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSC140A SuperHW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem. Consider the words “meilleur” and “mejor”. In English, both of these words mean “best”\n",
    "– one in French and the other in Spanish. Even if you don’t know how to speak either of these languages,\n",
    "you might be able to guess which word is Spanish and which is French based on the spelling. For example,\n",
    "the word “meilleur” looks more like a French word than a Spanish word due to it containing “ei” and ending\n",
    "in “eur”. On the other hand, the word “mejor” looks more like a Spanish word than a French word due to it\n",
    "containing “j” and ending in “or”. This suggests that there is some statistical structure in the words of these\n",
    "languages that we can use to build a machine learning model capable of distinguishing between French and\n",
    "Spanish without actually understanding the words themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal in this problem is to build a machine learning model that can take a word as input and predict\n",
    "whether it is Spanish or French."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• train_words: a list of n strings, each one of them a word (in either Spanish or French)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• train_labels: a list of n strings, each one of them either \"spanish\" or \"french\", indicating the\n",
    "language of the corresponding word in train_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•  test_words: a list of m strings, each one of them a word (in either Spanish or French)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function should return a list of m strings, each one of them either \"spanish\" or \"french\", indicating\n",
    "your classifier’s prediction for the language of the corresponding word in test_words.\n",
    "Your classify() function is responsible for training your machine learning model on the training data and\n",
    "then using that model to make predictions on the test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good choice of features is important. You might want to consider using the frequency of different\n",
    "letters or pairs of letters in each word as features. For example, one of your features might be whether\n",
    "“el” appears in the word. You do not need to create all of these features by hand – you can use Python\n",
    "to help you generate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don’t confuse training accuracy with test accuracy. It is possible to achieve 90%+ training accuracy\n",
    "on this data set, but that doesn’t mean your model will generalize well to the test set.\n",
    "2\n",
    "• Be careful to avoid overfitting! If you use too many features or too complex of a model, you may find\n",
    "that your model performs well on the training data but poorly on the test data.\n",
    "• Start with the simplest models first. We have learned some models in this class that can be implemented\n",
    "using only a couple lines of code (with numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def extract_word_dna(word):\n",
    "    dna = []\n",
    "    \n",
    "    for letter in word:\n",
    "        dna.append(f\"letter_{letter}\")\n",
    "    \n",
    "    for i in range(len(word) - 1):\n",
    "        double = word[i:i+2]\n",
    "        dna.append(f\"duo_{double}\")\n",
    "    \n",
    "    for tail_len in [1, 2, 3]:\n",
    "        if len(word) >= tail_len:\n",
    "            tail = word[-tail_len:]\n",
    "            dna.append(f\"tail_{tail}\")\n",
    "    \n",
    "    for head_len in [1, 2, 3]:\n",
    "        if len(word) >= head_len:\n",
    "            head = word[:head_len]\n",
    "            dna.append(f\"head_{head}\")\n",
    "            \n",
    "    size_bucket = min(len(word) // 2, 5)  \n",
    "    dna.append(f\"size_{size_bucket}\")\n",
    "    \n",
    "    return dna\n",
    "\n",
    "def build_language_fingerprints(words_list):\n",
    "    fingerprint = Counter()\n",
    "    \n",
    "    spanish_giveaways = [\"os\", \"ar\", \"er\", \"ir\", \"mente\", \"dad\", \"cion\", \"ll\", \"rr\", \"ia\", \"io\", \"ez\", \"ito\", \"ita\"]\n",
    "    french_giveaways = [\"eu\", \"ou\", \"ai\", \"ei\", \"au\", \"eau\", \"oi\", \"ie\", \"tion\", \"eux\", \"aux\", \"ez\", \"ais\", \"ment\"]\n",
    "    \n",
    "    for word in words_list:\n",
    "        for letter in word:\n",
    "            fingerprint[f\"letter_{letter}\"] += 1\n",
    "\n",
    "        for i in range(len(word) - 1):\n",
    "            double = word[i:i+2]\n",
    "            fingerprint[f\"duo_{double}\"] += 1\n",
    "            \n",
    "        for i in range(len(word) - 2):\n",
    "            triple = word[i:i+3]\n",
    "            fingerprint[f\"trio_{triple}\"] += 1\n",
    "\n",
    "        for tail_len in [1, 2, 3]:\n",
    "            if len(word) >= tail_len:\n",
    "                tail = word[-tail_len:]\n",
    "                fingerprint[f\"tail_{tail}\"] += 3  \n",
    "\n",
    "        for head_len in [1, 2, 3]:\n",
    "            if len(word) >= head_len:\n",
    "                head = word[:head_len]\n",
    "                fingerprint[f\"head_{head}\"] += 2  \n",
    "\n",
    "        size_bucket = min(len(word) // 2, 5)  \n",
    "        fingerprint[f\"size_{size_bucket}\"] += 1\n",
    "        \n",
    "        for pattern in spanish_giveaways:\n",
    "            if pattern in word:\n",
    "                fingerprint[f\"es_{pattern}\"] += 3\n",
    "                \n",
    "        for pattern in french_giveaways:\n",
    "            if pattern in word:\n",
    "                fingerprint[f\"fr_{pattern}\"] += 3\n",
    "    \n",
    "    return fingerprint\n",
    "\n",
    "def classify(train_words, train_labels, test_words):\n",
    "    espanol_words = [w.lower() for w, label in zip(train_words, train_labels) if label == \"spanish\"]\n",
    "    francais_words = [w.lower() for w, label in zip(train_words, train_labels) if label == \"french\"]\n",
    "    \n",
    "    word_count = len(train_words)\n",
    "    espanol_prob = len(espanol_words) / word_count\n",
    "    francais_prob = len(francais_words) / word_count\n",
    "    \n",
    "    espanol_prints = build_language_fingerprints(espanol_words)\n",
    "    francais_prints = build_language_fingerprints(francais_words)\n",
    "    \n",
    "    espanol_total = sum(espanol_prints.values())\n",
    "    francais_total = sum(francais_prints.values())\n",
    "    \n",
    "    vocab = set(list(espanol_prints.keys()) + list(francais_prints.keys()))\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for mystery_word in test_words:\n",
    "        mystery_word = mystery_word.lower()\n",
    "        \n",
    "        espanol_score = np.log(espanol_prob)\n",
    "        francais_score = np.log(francais_prob)\n",
    "        \n",
    "        word_dna = extract_word_dna(mystery_word)\n",
    "        \n",
    "        for feature in word_dna:\n",
    "            if feature in espanol_prints:\n",
    "                espanol_score += np.log((espanol_prints[feature] + 1) / (espanol_total + vocab_size))\n",
    "            else:\n",
    "                espanol_score += np.log(1 / (espanol_total + vocab_size))\n",
    "            \n",
    "            if feature in francais_prints:\n",
    "                francais_score += np.log((francais_prints[feature] + 1) / (francais_total + vocab_size))\n",
    "            else:\n",
    "                francais_score += np.log(1 / (francais_total + vocab_size))\n",
    "        \n",
    "        francais_score += 0.06\n",
    "        \n",
    "        if espanol_score > francais_score:\n",
    "            results.append(\"spanish\")\n",
    "        else:\n",
    "            results.append(\"french\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy: 86.67%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy(true_labels, predicted_labels):\n",
    "    correct = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)\n",
    "    return correct / len(true_labels) * 100  # Return as percentage\n",
    "\n",
    "# Create a validation split from your training data\n",
    "import random\n",
    "\n",
    "def train_val_split(words, labels, val_size=0.2):\n",
    "    # Create paired data\n",
    "    data = list(zip(words, labels))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # Calculate split point\n",
    "    split_idx = int(len(data) * (1 - val_size))\n",
    "    \n",
    "    # Split data\n",
    "    train_data = data[:split_idx]\n",
    "    val_data = data[split_idx:]\n",
    "    \n",
    "    # Unzip the data\n",
    "    train_words, train_labels = zip(*train_data)\n",
    "    val_words, val_labels = zip(*val_data)\n",
    "    \n",
    "    return list(train_words), list(train_labels), list(val_words), list(val_labels)\n",
    "\n",
    "# Load your training data\n",
    "import csv\n",
    "\n",
    "def load_data(file_path):\n",
    "    words = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header if present\n",
    "        for row in reader:\n",
    "            words.append(row[0])\n",
    "            labels.append(row[1])\n",
    "    \n",
    "    return words, labels\n",
    "\n",
    "# Example usage:\n",
    "words, labels = load_data('train.csv')  # Path to the dataset provided in the assignment\n",
    "train_words, train_labels, val_words, val_labels = train_val_split(words, labels)\n",
    "\n",
    "# Train on train_words/train_labels and predict on val_words\n",
    "predictions = classify(train_words, train_labels, val_words)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = calculate_accuracy(val_labels, predictions)\n",
    "print(f\"Model accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracies: [81.66666666666667, 84.16666666666667, 86.25, 87.5, 86.66666666666667]\n",
      "Average accuracy: 85.25%\n"
     ]
    }
   ],
   "source": [
    "def cross_validate(words, labels, k=5):\n",
    "    # Create paired data\n",
    "    data = list(zip(words, labels))\n",
    "    random.seed(42)  # For reproducibility\n",
    "    random.shuffle(data)\n",
    "    \n",
    "    # Calculate fold size\n",
    "    fold_size = len(data) // k\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        # Select validation fold\n",
    "        start = i * fold_size\n",
    "        end = start + fold_size if i < k-1 else len(data)\n",
    "        val_data = data[start:end]\n",
    "        train_data = data[:start] + data[end:]\n",
    "        \n",
    "        # Split into words and labels\n",
    "        train_words, train_labels = zip(*train_data)\n",
    "        val_words, val_labels = zip(*val_data)\n",
    "        \n",
    "        # Train and predict\n",
    "        predictions = classify(list(train_words), list(train_labels), list(val_words))\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = calculate_accuracy(val_labels, predictions)\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    return accuracies\n",
    "\n",
    "# Run cross-validation\n",
    "accuracies = cross_validate(words, labels)\n",
    "print(f\"Cross-validation accuracies: {accuracies}\")\n",
    "print(f\"Average accuracy: {sum(accuracies)/len(accuracies):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(train_words, train_labels, test_words, test_labels):\n",
    "    \"\"\"Generate confusion matrix to identify error patterns\"\"\"\n",
    "    predictions = classify(train_words, train_labels, test_words)\n",
    "    \n",
    "    # Initialize counts\n",
    "    tp = fp = tn = fn = 0\n",
    "    for pred, true in zip(predictions, test_labels):\n",
    "        if pred == \"spanish\" and true == \"spanish\":\n",
    "            tp += 1\n",
    "        elif pred == \"spanish\" and true == \"french\":\n",
    "            fp += 1\n",
    "        elif pred == \"french\" and true == \"french\":\n",
    "            tn += 1\n",
    "        elif pred == \"french\" and true == \"spanish\":\n",
    "            fn += 1\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (tp + tn) / len(test_labels)\n",
    "    precision_spanish = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall_spanish = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision_spanish\": precision_spanish,\n",
    "        \"recall_spanish\": recall_spanish,\n",
    "        \"confusion_matrix\": [[tp, fn], [fp, tn]]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_edge_cases(train_words, train_labels):\n",
    "    \"\"\"Test classifier on specially crafted challenging cases\"\"\"\n",
    "    edge_cases = [\n",
    "        # Words that could be either language\n",
    "        \"animal\", \"central\", \"radio\", \"normal\",\n",
    "        \n",
    "        # Very short words\n",
    "        \"no\", \"si\", \"la\", \"le\",\n",
    "        \n",
    "        # Words with language-specific patterns\n",
    "        \"español\", \"français\", \"biblioteca\", \"bibliothèque\",\n",
    "    ]\n",
    "    \n",
    "    predictions = classify(train_words, train_labels, edge_cases)\n",
    "    for word, pred in zip(edge_cases, predictions):\n",
    "        print(f\"'{word}' classified as {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
